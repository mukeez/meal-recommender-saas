"""Base class for AI services"""

from typing import Any
import openai
from app.core.config import settings
from openai import OpenAI


class LLMServiceError(Exception):
    """Custom exception for AI service errors."""

    pass


class BaseLLMService:
    """
    Base class for AI services.

    Provides template methods for generating response from AI while allowing subclasses to override or extend any part as needed.
    """

    def __init__(self):
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.MODEL_NAME

    def _build_prompt(self, request: Any) -> str:
        """
        Construct the prompt string. Subclasses should override this.
        """
        raise NotImplementedError("Subclasses must implement _build_prompt")

    def _parse_response(self, content: str) -> Any:
        """
        Parse raw AI output into structured objects. Subclasses should override this.
        """
        raise NotImplementedError("Subclasses must implement _parse_response")

    async def _send_request(
        self,
        system_prompt,
        prompt: str,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """
        Send a request to the AI service and return the raw response.
        Args:
            system_prompt: The system prompt to set the context for the AI.
            prompt: The user input to generate a response for.
            max_tokens: Maximum number of tokens in the response.
            temperature: Sampling temperature for response variability.
        Returns:
            The raw response content from the AI service.
        """
        try:
            response = self.client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {"role": "user", "content": prompt},
                ],
                response_format={"type": "json_object"},
                max_tokens=max_tokens,
                temperature=temperature,
            )
            return response.choices[0].message.content
        except openai.OpenAIError as e:
            raise LLMServiceError(f"OpenAI API error: {e}")

    async def generate_response(
        self,
        system_prompt: str,
        request: Any,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        **kwargs
    ) -> Any:
        """
        Template method: build prompt, send request, parse response.

        Args:
            request: Any
        Returns:
            A parsed response generated by the AI service
        """
        should_parse = kwargs.get("should_parse", True) 

        prompt = self._build_prompt(request)
        raw = await self._send_request(
            system_prompt=system_prompt,
            prompt=prompt,
            # max_tokens=max_tokens,
            temperature=temperature,
        )
        if not should_parse:
            return raw
        return self._parse_response(raw)
